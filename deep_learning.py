# -*- coding: utf-8 -*-
"""deep-learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWSXd3H6ReK6QZCIr3hxArb1hjSoMCLD

# Deep learning project
Here comes important information about the project**...**
"""

# configuration
cfg = {
    "base_model": {
        "name": "ViT-B-32",
        "weights": "laion2b_s34b_b79k",
    }
}

# installing packages, use specific versions
# !pip3 install torch==2.6.0 torchvision==0.21.0 open_clip_torch==2.32.0 wandb==0.19.10

import re
import torch
import torchvision
import open_clip
from tqdm import tqdm
import time
import wandb

"""## Initialize wandb"""

wandb_run = wandb.init(entity="mhevizi-unitn", project="Deep learning project", config=cfg)

"""## Define data loaders; base, novel splits"""

CLASS_NAMES = [
    "pink primrose",
    "hard-leaved pocket orchid",
    "canterbury bells",
    "sweet pea",
    "english marigold",
    "tiger lily",
    "moon orchid",
    "bird of paradise",
    "monkshood",
    "globe thistle",
    "snapdragon",
    "colt's foot",
    "king protea",
    "spear thistle",
    "yellow iris",
    "globe-flower",
    "purple coneflower",
    "peruvian lily",
    "balloon flower",
    "giant white arum lily",
    "fire lily",
    "pincushion flower",
    "fritillary",
    "red ginger",
    "grape hyacinth",
    "corn poppy",
    "prince of wales feathers",
    "stemless gentian",
    "artichoke",
    "sweet william",
    "carnation",
    "garden phlox",
    "love in the mist",
    "mexican aster",
    "alpine sea holly",
    "ruby-lipped cattleya",
    "cape flower",
    "great masterwort",
    "siam tulip",
    "lenten rose",
    "barbeton daisy",
    "daffodil",
    "sword lily",
    "poinsettia",
    "bolero deep blue",
    "wallflower",
    "marigold",
    "buttercup",
    "oxeye daisy",
    "common dandelion",
    "petunia",
    "wild pansy",
    "primula",
    "sunflower",
    "pelargonium",
    "bishop of llandaff",
    "gaura",
    "geranium",
    "orange dahlia",
    "pink-yellow dahlia?",
    "cautleya spicata",
    "japanese anemone",
    "black-eyed susan",
    "silverbush",
    "californian poppy",
    "osteospermum",
    "spring crocus",
    "bearded iris",
    "windflower",
    "tree poppy",
    "gazania",
    "azalea",
    "water lily",
    "rose",
    "thorn apple",
    "morning glory",
    "passion flower",
    "lotus",
    "toad lily",
    "anthurium",
    "frangipani",
    "clematis",
    "hibiscus",
    "columbine",
    "desert-rose",
    "tree mallow",
    "magnolia",
    "cyclamen",
    "watercress",
    "canna lily",
    "hippeastrum",
    "bee balm",
    "ball moss",
    "foxglove",
    "bougainvillea",
    "camellia",
    "mallow",
    "mexican petunia",
    "bromelia",
    "blanket flower",
    "trumpet creeper",
    "blackberry lily",
]


def get_data(data_dir="./data", transform=None):
    """Load Flowers102 train, validation and test sets.
    Args:
        data_dir (str): Directory where the dataset will be stored.
        transform (torch.Compose)
    Returns:
        tuple: A tuple containing the train, validation, and test sets.
    """
    train = torchvision.datasets.Flowers102(
        root=data_dir, split="train", download=True, transform=transform
    )
    val = torchvision.datasets.Flowers102(
        root=data_dir, split="val", download=True, transform=transform
    )
    test = torchvision.datasets.Flowers102(
        root=data_dir, split="test", download=True, transform=transform
    )
    return train, val, test


def base_novel_categories(dataset):
    # set returns the unique set of all dataset classes
    all_classes = set(dataset._labels)
    # and let's count them
    num_classes = len(all_classes)

    # here list(range(num_classes)) returns a list from 0 to num_classes - 1
    # then we slice the list in half and generate base and novel category lists
    base_classes = list(range(num_classes))[: num_classes // 2]
    novel_classes = list(range(num_classes))[num_classes // 2 :]
    return base_classes, novel_classes


def split_data(dataset, base_classes):
    # these two lists will store the sample indexes
    base_categories_samples = []
    novel_categories_samples = []

    # we create a set of base classes to compute the test below in O(1)
    # this is optional and can be removed
    base_set = set(base_classes)

    # here we iterate over sample labels and also get the correspondent sample index
    for sample_id, label in enumerate(dataset._labels):
        if label in base_set:
            base_categories_samples.append(sample_id)
        else:
            novel_categories_samples.append(sample_id)

    # here we create the dataset subsets
    # the torch Subset is just a wrapper around the dataset
    # it simply stores the subset indexes and the original dataset (your_subset.dataset)
    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it
    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset
    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)
    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)
    return base_dataset, novel_dataset


"""## Define training and testing loops"""


@torch.no_grad()
def eval(model, dataset, categories, batch_size, device, tokenizer, label=""):
    model.eval()

    # Remap labels into a contiguous set starting from zero
    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}

    # here we apply the standard CLIP template used for oxford flowers to all categories
    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)
    text_inputs = tokenizer(
        [f"a photo of a {CLASS_NAMES[c]}, a type of flower." for c in categories]
    ).to(device)

    # we can encode the text features once as they are shared for all images
    # therefore we do it outside the evaluation loop
    text_features = model.encode_text(text_inputs)
    # and here we normalize them (standard pratice with CLIP)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # simple dataloader creation
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )

    # here we store the number of correct predictions we will make
    correct_predictions = 0
    for image, target in tqdm(dataloader, desc=label):
        # base categories range from 0 to 50, whil novel ones from 51 to 101
        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions
        # Map targets in contiguous set starting from zero
        # Labels needs to be .long() in pytorch
        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()

        image = image.to(device)
        target = target.to(device)

        # forward image through CLIP image encoder
        image_features = model.encode_image(image)
        # and normalize
        image_features /= image_features.norm(dim=-1, keepdim=True)

        # here cosine similarity between image and text features and keep the argmax for every row (every image)
        predicted_class = (image_features @ text_features.T).argmax(dim=-1)
        # now we check which are correct, and sum them (False == 0, True == 1)
        correct_predictions += (predicted_class == target).sum().item()

    # and now we compute the accuracy
    accuracy = correct_predictions / len(dataset)
    return accuracy


def harmonic_mean(base_accuracy, novel_accuracy):
    numerator = 2
    denominator = 1 / base_accuracy + 1 / novel_accuracy
    hm = numerator / denominator
    return hm


def train_loop(dataloader, model, loss_fn, optimizer, batch_size):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        wandb_run.log({"train_loss": loss.item()})

        if batch % 100 == 0:
            loss, current = loss.item(), batch * batch_size + len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


@torch.no_grad()
def test_loop(dataloader, model, loss_fn):
    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    for X, y in dataloader:
        pred = model(X)
        test_loss += loss_fn(pred, y).item()
        correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
    wandb_run.log({"val_acc": correct, "val_loss": test_loss})


"""## Define model, loss"""


class MetaNet(torch.nn.Module):
    def __init__(
        self,
        device: str,
        image_embedding_size: int = 768,
        text_embedding_size: int = 512,
        hidden_reduction_factor: int = 16,
    ) -> None:
        super().__init__()
        hidden_layer_size = image_embedding_size // hidden_reduction_factor
        self.lin_stack = torch.nn.Sequential(
            torch.nn.Linear(image_embedding_size, hidden_layer_size),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_layer_size, text_embedding_size),
        )

    def forward(self, x):
        x = torch.nn.Flatten(x)
        logits = self.lin_stack(x)
        return logits


class CustomModel(torch.nn.Module):
    def __init__(
        self,
        device: str,
        text_embedding_size: int = 512,
        context_size: int = 16,
        class_location="end",
    ) -> None:
        super().__init__()
        assert (
            class_location == "end"
            "We only support class location at the end, feel free to implement others :)"
        )
        result = open_clip.create_model_from_pretrained(
            model_name=cfg["base_model"]["name"],
            pretrained=cfg["base_model"]["weights"],
            device=device,
            return_transform=True,
        )
        assert isinstance(result, tuple)
        self.clip, self.preprocess = result
        self.tokenizer = open_clip.get_tokenizer(cfg["base_model"]["name"])
        for p in self.clip.parameters():
            p.requires_grad = False

        self.meta_net = MetaNet(device)
        # prompt is initialized from normal distribution with mean 0 and std 0.02 like in the paper
        self.unified_context = torch.nn.Parameter(
            0.02 * torch.randn(context_size, text_embedding_size, device=device), requires_grad=True
        )
        self.text_inputs = self._generate_text_input()

    def forward(self, images):
        _, text_logits = self.clip.get_logits(images, self.text_inputs)
        return text_logits

    def _generate_text_input(self):
        tokenized_classes = [
            [self.tokenizer.sot_token_id]
            + self.tokenizer.encode(re.sub("-", " ", class_name))
            + self.tokenizer.eot_token_id
            for class_name in CLASS_NAMES
        ]
        tokenized_class_lens = [len(tokenized_class) for tokenized_class in tokenized_classes]

        result = torch.zeros((len(tokenized_classes), 1 + max(tokenized_class_lens)))
        for i, tokenized_class in enumerate(tokenized_classes):
            pass

        return result


def create_custom_model(device):
    model = CustomModel(device)
    preprocess = model.preprocess
    tokenizer = open_clip.get_tokenizer(cfg["base_model"]["name"])
    return model, preprocess, tokenizer


def create_base_model(device):
    result = open_clip.create_model_from_pretrained(
        model_name=cfg["base_model"]["name"],
        pretrained=cfg["base_model"]["weights"],
        device=device,
        return_transform=True,
    )
    assert isinstance(result, tuple)
    model, preprocess = result
    tokenizer = open_clip.get_tokenizer(cfg["base_model"]["name"])
    return model, preprocess, tokenizer


"""## Initialize model"""

device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"Using {device} device")

base_model, base_preprocess, base_tokenizer = create_base_model(device)
custom_model, custom_preprocess, custom_tokenizer = create_custom_model(device)
print(base_model)

"""# Load and prepare data

"""

# get the three datasets
train_set, val_set, test_set = get_data(transform=base_preprocess)

# split classes into base and novel
base_classes, novel_classes = base_novel_categories(train_set)

# split the three datasets
train_base, _ = split_data(train_set, base_classes)
val_base, _ = split_data(val_set, base_classes)
test_base, test_novel = split_data(test_set, base_classes)

"""## Train model"""

### here will be the training and testing loop defined

"""## Evaluate zero shot accuracy"""

base_accuracy = eval(
    model=base_model,
    dataset=test_base,
    categories=base_classes,
    batch_size=128,
    device=device,
    tokenizer=base_tokenizer,
    label="üß† Zero-shot evaluation on Base Classes",
)
novel_accuracy = eval(
    model=base_model,
    dataset=test_novel,
    categories=novel_classes,
    batch_size=128,
    device=device,
    tokenizer=base_tokenizer,
    label="üß† Zero-shot evaluation on Novel Classes",
)

print()
print(f"üîç Base classes accuracy: {base_accuracy * 100:.2f}%")
print(f"üîç Novel classes accuracy: {novel_accuracy * 100:.2f}%")
print(f"üîç Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy) * 100:.2f}%")
wandb_run.finish()
