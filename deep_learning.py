# -*- coding: utf-8 -*-
"""deep-learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWSXd3H6ReK6QZCIr3hxArb1hjSoMCLD

# Deep learning project
Here comes important information about the project**...**
"""

# TO DO - HELIP:
# 1. Once HELIP is validated by the group, take out all the comments that containt the word "HELIP" (except the ones in the config)
# 2. To make sure HELIP is actually doing something, the HNML in the logs should be non-zero.
# 3. The loss should be lower than without HELIP.

# configuration
cfg = {
    "base_model": {
        "name": "ViT-B-32",
        "weights": "laion2b_s34b_b79k",
    },
    "trainer": {
        "batch_size": 4,  # 32 is the default batch size for the ViT-B-32 model but it was crashing my local machine
        "epochs": 3,
        "lr": 0.002,          # test 
        "weight_decay": 0.05, # test
        "warmup_epochs": 1,   # Possibly we'll need more
        "patience": 5,
        "checkpoint_dir": "./checkpoints",
        "cocoop": {
            "n_ctx": 8,     # Reduced from 16 to save memory
            "ctx_init": "", # empty string for random initialization (FOR NOW)
            "prec": "fp16"  # Use fp16 precision to save memory
        }
    },
    "input": {
        "size": [224, 224]  # Input image size
    },
    "data": {
        "data_dir": "./data",
        "num_workers": 0,    # Enable parallel data loading when running in cloud, use 2 or 4
        "pin_memory": False  # Idem
    },
    # HELIP configs
    "helip": {
        "enabled": True,
        "k": 5,                # Number of hard pairs to mine per sample, diminishing returns, in the paper they also suggested 3
        "p": 20,               # Number of random pairs to sample
        "alpha": 0.25,         # Margin parameter for HNML (Hard Negative Mining Loss)
        "lambda_hnml": 0.5,    # Weight for HNML in the combined loss - 
                               # TO DO - 1: find a better value for key hyperparam 
                               # TO DO - 2: balance this lambda with other regularizers lambdas, if all high, we'll underfit 
                               # (cross-modal consistency, augmentation, etc.)
        "mining_freq": 5,      # Mine hard pairs every N epochs (to avoid mining each epoch)
        "cache_embeddings": True  # Whether to cache embeddings between epochs, to avoid recomputing them
    }
}

# installing packages, use specific versions
# TO DO: Make sure HELIP requirements are well included here
# !pip3 install torch==2.6.0 torchvision==0.21.0 open_clip_torch==2.32.0 wandb==0.19.10

import os
from collections import OrderedDict
import re
import torch
import torchvision
import open_clip
from tqdm import tqdm
import time
import wandb
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from pathlib import Path
import logging

"""## Initialize wandb"""

# Setup some logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("cocoop_training")

# Create checkpoint directory
os.makedirs(cfg["trainer"]["checkpoint_dir"], exist_ok=True)

# Initialize a more complete version of wandb (with configuration)
wandb_run = wandb.init(
    entity="mhevizi-unitn", 
    project="Deep learning project", 
    config=cfg,
    name=f"CoCoOp_{cfg['base_model']['name']}_{cfg['trainer']['lr']}",
    tags=["cocoop", cfg["base_model"]["name"]]
)

"""## Define data loaders; base, novel splits"""

CLASS_NAMES = [
    "pink primrose",
    "hard-leaved pocket orchid",
    "canterbury bells",
    "sweet pea",
    "english marigold",
    "tiger lily",
    "moon orchid",
    "bird of paradise",
    "monkshood",
    "globe thistle",
    "snapdragon",
    "colt's foot",
    "king protea",
    "spear thistle",
    "yellow iris",
    "globe-flower",
    "purple coneflower",
    "peruvian lily",
    "balloon flower",
    "giant white arum lily",
    "fire lily",
    "pincushion flower",
    "fritillary",
    "red ginger",
    "grape hyacinth",
    "corn poppy",
    "prince of wales feathers",
    "stemless gentian",
    "artichoke",
    "sweet william",
    "carnation",
    "garden phlox",
    "love in the mist",
    "mexican aster",
    "alpine sea holly",
    "ruby-lipped cattleya",
    "cape flower",
    "great masterwort",
    "siam tulip",
    "lenten rose",
    "barbeton daisy",
    "daffodil",
    "sword lily",
    "poinsettia",
    "bolero deep blue",
    "wallflower",
    "marigold",
    "buttercup",
    "oxeye daisy",
    "common dandelion",
    "petunia",
    "wild pansy",
    "primula",
    "sunflower",
    "pelargonium",
    "bishop of llandaff",
    "gaura",
    "geranium",
    "orange dahlia",
    "pink-yellow dahlia?",
    "cautleya spicata",
    "japanese anemone",
    "black-eyed susan",
    "silverbush",
    "californian poppy",
    "osteospermum",
    "spring crocus",
    "bearded iris",
    "windflower",
    "tree poppy",
    "gazania",
    "azalea",
    "water lily",
    "rose",
    "thorn apple",
    "morning glory",
    "passion flower",
    "lotus",
    "toad lily",
    "anthurium",
    "frangipani",
    "clematis",
    "hibiscus",
    "columbine",
    "desert-rose",
    "tree mallow",
    "magnolia",
    "cyclamen",
    "watercress",
    "canna lily",
    "hippeastrum",
    "bee balm",
    "ball moss",
    "foxglove",
    "bougainvillea",
    "camellia",
    "mallow",
    "mexican petunia",
    "bromelia",
    "blanket flower",
    "trumpet creeper",
    "blackberry lily",
]

# Added space for augmentations
# Transforms for training (WITH augmentation) and validation/test (WITHOUT augmentation)
def get_data(data_dir="./data", train_transform=None, test_transform=None):

    logger.info(f"Loading Flowers102 dataset from {data_dir}")
    
    train = torchvision.datasets.Flowers102(
        root=data_dir, split="train", download=True, transform=train_transform
    )
    val = torchvision.datasets.Flowers102(
        root=data_dir, split="val", download=True, transform=test_transform
    )
    test = torchvision.datasets.Flowers102(
        root=data_dir, split="test", download=True, transform=test_transform
    )
    
    logger.info(f"Dataset loaded: {len(train)} train, {len(val)} val, {len(test)} test")
    return train, val, test


def base_novel_categories(dataset):
    # set returns the unique set of all dataset classes
    all_classes = set(dataset._labels)
    # and let's count them
    num_classes = len(all_classes)

    # here list(range(num_classes)) returns a list from 0 to num_classes - 1
    # then we slice the list in half and generate base and novel category lists
    base_classes = list(range(num_classes))[: num_classes // 2]
    novel_classes = list(range(num_classes))[num_classes // 2 :]
    return base_classes, novel_classes


def split_data(dataset, base_classes):
    # these two lists will store the sample indexes
    base_categories_samples = []
    novel_categories_samples = []

    # we create a set of base classes to compute the test below in O(1)
    # this is optional and can be removed
    base_set = set(base_classes)

    # here we iterate over sample labels and also get the correspondent sample index
    for sample_id, label in enumerate(dataset._labels):
        if label in base_set:
            base_categories_samples.append(sample_id)
        else:
            novel_categories_samples.append(sample_id)

    # here we create the dataset subsets
    # the torch Subset is just a wrapper around the dataset
    # it simply stores the subset indexes and the original dataset (your_subset.dataset)
    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it
    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset
    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)
    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)
    return base_dataset, novel_dataset


"""## Define training and testing loops"""

# HELIP

# Initialize Hard Pair Mining. We pick the CustomCLIP model as the base model, data, device and configs
class HardPairMining:
    def __init__(self, custom_clip_model, dataset, device, cfg):
        
        self.model = custom_clip_model  # CustomCLIP model with prompt learner
        self.dataset = dataset
        self.device = device
        self.k = cfg["helip"]["k"]
        self.p = cfg["helip"]["p"]
        self.alpha = cfg["helip"]["alpha"]
        
        # Storage for cached embeddings and hard pairs
        self.cached_embeddings = None
        self.cached_hard_pairs = {}
        self.cache_embeddings = cfg["helip"]["cache_embeddings"]
        self.last_mining_epoch = 0
    
    # Compute embeddings for pairs in the dataset using CoCoOp's prompt learning mechanism
    def _compute_pair_embeddings(self, sample_indices=None):
        if sample_indices is None:
            dataloader = DataLoader(
                self.dataset, 
                batch_size=4,  # TO DO: find proper batch size (for now, a very small one to avoid memory issues
                shuffle=False, 
                num_workers=cfg["data"]["num_workers"]
            )
        else:
            # Create a subset with specific indices
            subset = torch.utils.data.Subset(self.dataset, sample_indices)
            dataloader = DataLoader(
                subset, 
                batch_size=4, # TO DO: Idem above
                shuffle=False, 
                num_workers=cfg["data"]["num_workers"]
            )
        
        all_image_features = []
        all_text_features = []
        all_labels = []
        
        with torch.no_grad():
            self.model.eval()
            for images, labels in tqdm(dataloader, desc="Computing embeddings for HELIP"):
                images = images.to(self.device)
                
                # Run the full CoCoOp forward pass to get text features with the meta-network
                # Important to use CoCoOp's prompt mechanism properly
                logits = self.model(images)  # This will populate self.model.text_features
                
                # Get image features
                image_features = self.model.image_encoder(images.type(self.model.dtype))
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # Now extract the text features which have been stored
                # The model should compute text features for all classes
                if self.model.text_features is not None:
                    # For CoCoOp, text_features can have shape [batch_size * num_classes, feature_dim]
                    # We need to ensure we only take features for this batch
                    batch_size = image_features.size(0)
                    
                    # Handle different text_features shapes based on CoCoOp implementation
                    if self.model.text_features.size(0) > batch_size:
                        # If text_features contains features for all classes, get just batch_size entries
                        text_features = self.model.text_features[:batch_size]
                    else:
                        text_features = self.model.text_features
                    
                    # Store the embeddings
                    all_image_features.append(image_features.cpu())
                    all_text_features.append(text_features.cpu())
                    all_labels.append(labels)
                else:
                    logger.warning("No text features found after forward pass. Skipping batch.")
        
        # Only create embeddings if we have data
        if not all_image_features or not all_text_features:
            logger.error("No embeddings were collected during mining. Check model implementation.")
            return None
            
        # Create the embeddings dictionary
        embeddings = {
            'image_features': torch.cat(all_image_features),
            'text_features': torch.cat(all_text_features),
            'labels': torch.cat(all_labels)
        }
        
        # Log shapes for debugging
        logger.info(f"Computed embeddings - Images: {embeddings['image_features'].shape}, Text: {embeddings['text_features'].shape}")
        
        # Verify sizes match
        if embeddings['image_features'].size(0) != embeddings['text_features'].size(0):
            logger.warning(f"Mismatch in number of samples: image={embeddings['image_features'].size(0)}, text={embeddings['text_features'].size(0)}")
            logger.warning("Attempting to fix by truncating to shorter dimension")
            min_size = min(embeddings['image_features'].size(0), embeddings['text_features'].size(0))
            embeddings['image_features'] = embeddings['image_features'][:min_size]
            embeddings['text_features'] = embeddings['text_features'][:min_size]
            embeddings['labels'] = embeddings['labels'][:min_size]
        
        return embeddings
    
    # Find hard pairs for the given dataset indices using the original HELIP approach
    def mine_hard_pairs(self, dataset_indices, epoch):
        # Check if we need to mine new pairs based on epoch frequency
        if epoch // cfg["helip"]["mining_freq"] == self.last_mining_epoch // cfg["helip"]["mining_freq"] and self.cached_hard_pairs:
            logger.info(f"Using cached hard pairs from epoch {self.last_mining_epoch}")
            return self.cached_hard_pairs
            
        # Compute or load embeddings
        if self.cached_embeddings is None or not self.cache_embeddings:
            logger.info("Computing embeddings for hard pair mining")
            self.cached_embeddings = self._compute_pair_embeddings()
        
        hard_pairs = {}
        
        # Get all embeddings
        all_image_features = self.cached_embeddings['image_features']
        all_text_features = self.cached_embeddings['text_features']
        
        # For each dataset index, find hard pairs
        for target_idx in tqdm(dataset_indices, desc=f"Mining hard pairs for epoch {epoch}"):
            # Convert target_idx to int if it's a tensor
            target_idx_int = target_idx.item() if hasattr(target_idx, 'item') else target_idx
            
            # Get target embeddings
            target_image_emb = all_image_features[target_idx].to(self.device)
            target_text_emb = all_text_features[target_idx].to(self.device)
            
            # Randomly sample p candidate pairs (as in original HELIP)
            total_samples = len(all_image_features)
            random_indices = torch.randperm(total_samples)[:self.p]
            
            # Calculate agreement scores (how well other pairs match with target pair)
            agreement_scores = []
            
            for idx in random_indices:
                if idx == target_idx:
                    continue
                    
                candidate_image_emb = all_image_features[idx].to(self.device)
                candidate_text_emb = all_text_features[idx].to(self.device)
                
                # Agreement score as in HELIP paper
                # Using torch.matmul for scalar products to avoid transpose warning
                i2t_score = torch.matmul(target_image_emb.flatten(), candidate_text_emb.flatten())
                t2i_score = torch.matmul(candidate_image_emb.flatten(), target_text_emb.flatten())
                score = i2t_score + t2i_score
                
                # Convert idx to int if it's a tensor
                idx_int = idx.item() if hasattr(idx, 'item') else idx
                agreement_scores.append((idx_int, score.item()))
            
            # Sort by score (lower is harder) and get top-k hard pairs
            hard_pairs_indices = sorted(agreement_scores, key=lambda x: x[1])[:self.k]
            hard_pairs[target_idx_int] = [idx for idx, _ in hard_pairs_indices]
        
        # Cache results
        self.cached_hard_pairs = hard_pairs
        self.last_mining_epoch = epoch
        
        return hard_pairs

# Compute Hard Negative Margin Loss as described in HELIP's paper
def hard_negative_margin_loss(image_features, # Normalized image features [batch_size, feature_dim]
                              text_features, # Normalized text features [batch_size, feature_dim]
                              hard_pairs_indices, # Dictionary mapping indices to lists of hard pair indices
                              alpha=0.05 # Margin parameter (originally set at 0.25, flexibilized, should be tested)
                              ):
    batch_size = image_features.shape[0]
    device = image_features.device
    
    # Calculate similarity matrix between all images and texts
    sim_matrix = image_features @ text_features.T
    
    # Define positive pair similarities (diagonal elements)
    pos_idx = torch.arange(batch_size, device=device)
    pos_sim = sim_matrix[pos_idx, pos_idx]
    
    # Initialize loss
    hnml_loss = torch.tensor(0.0, device=device)
    count = 0
    
    # Process each sample in the batch
    for i in range(batch_size):
        # Convert sample_idx to int if it's a tensor
        sample_idx = i
        
        if sample_idx not in hard_pairs_indices or not hard_pairs_indices[sample_idx]:
            continue
        
        # Get hard indices and convert to tensor
        hard_indices_list = hard_pairs_indices[sample_idx]
        hard_indices = torch.tensor(hard_indices_list, device=device)
        
        # Find indices that are in the current batch
        batch_mask = hard_indices < batch_size
        if not batch_mask.any():
            continue
            
        hard_indices = hard_indices[batch_mask]
        
        # Get similarities with hard negative pairs
        hard_neg_sim_i2t = sim_matrix[i, hard_indices]
        hard_neg_sim_t2i = sim_matrix[hard_indices, i]
        
        # Calculate hinge loss with margin for each hard negative
        i2t_loss = torch.sum(torch.clamp(hard_neg_sim_i2t - pos_sim[i] + alpha, min=0))
        t2i_loss = torch.sum(torch.clamp(hard_neg_sim_t2i - pos_sim[i] + alpha, min=0))
        
        hnml_loss += (i2t_loss + t2i_loss)
        count += len(hard_indices) * 2
    
    # Normalize by actual number of pairs processed, or return zero if no pairs
    if count > 0:
        return hnml_loss / count
    return hnml_loss




def train_loop(dataloader, model, optimizer, epoch, device, scheduler=None, 
                # HELIP
                use_helip=False, # Boolean flag to use or not HELIP
                hard_pair_miner=None, # Hard pair mining module
                lambda_hnml=0.5 # Weight for hard negative margin loss
                # End of HELIP
                ):

    model.train()
    total_loss = 0
    total_contrastive_loss = 0
    total_hnml_loss = 0
    
    # Progress bar for training
    progress_bar = tqdm(dataloader, desc=f"Train Epoch {epoch}")
    
    # Initialize hard_pairs as empty dict by default
    hard_pairs = {}
    
    # If HELIP is enabled, mine hard pairs for this epoch
    if use_helip and hard_pair_miner is not None:
        # Get all dataset indices for current batch
        dataset_indices = dataloader.dataset.indices if hasattr(dataloader.dataset, 'indices') else list(range(len(dataloader.dataset)))
        
        # Mine hard pairs for this epoch
        hard_pairs = hard_pair_miner.mine_hard_pairs(dataset_indices, epoch)
    # End of HELIP
    
    # Training loop
    for batch_idx, (images, targets) in enumerate(progress_bar):
        # Move data to device
        images, targets = images.to(device), targets.to(device)
        
        # Zero gradients
        optimizer.zero_grad()
        
        # Standard CoCoOp forward pass which returns cross-entropy loss
        contrastive_loss = model(images, targets)
        
        # Convert non-scalar loss to scalar for backprop - this is essential
        if isinstance(contrastive_loss, torch.Tensor) and contrastive_loss.dim() > 0:
            logger.info(f"Converting contrastive loss of shape {contrastive_loss.shape} to scalar for backprop")
            contrastive_loss = contrastive_loss.mean()
        
        # Safely convert to scalar for logging
        try:
            # Check if contrastive_loss is a scalar or can be converted to one
            if isinstance(contrastive_loss, torch.Tensor) and contrastive_loss.numel() == 1:
                cont_loss_val = contrastive_loss.item()
            else:
                # If it's not a scalar, log a warning and use a dummy value
                logger.warning(f"Contrastive loss is not a scalar (shape: {contrastive_loss.shape if hasattr(contrastive_loss, 'shape') else 'unknown'}). Using mean for logging.")
                cont_loss_val = contrastive_loss.mean().item() if isinstance(contrastive_loss, torch.Tensor) else 0.0
        except Exception as e:
            logger.warning(f"Error converting contrastive loss to scalar: {e}. Using 0.0 for logging.")
            cont_loss_val = 0.0
            
        total_contrastive_loss += cont_loss_val
        
        # If HELIP is enabled, compute additional hard negative margin loss
        if use_helip and hard_pair_miner is not None and lambda_hnml > 0:
            # Get batch indices
            batch_start = batch_idx * dataloader.batch_size
            batch_end = min(batch_start + dataloader.batch_size, len(dataset_indices))
            batch_indices = list(range(batch_start, batch_end))
            
            # Get hard pairs for current batch samples
            batch_hard_pairs = {}
            for i, orig_idx in enumerate(batch_indices):
                if not hasattr(dataloader.dataset, 'indices'):
                    # If dataset doesn't have indices attribute (direct dataset)
                    dataset_idx = orig_idx
                else:
                    # If dataset is a Subset (has indices attribute)
                    dataset_idx = dataset_indices[orig_idx]
                
                dataset_idx_int = dataset_idx.item() if hasattr(dataset_idx, 'item') else dataset_idx
                
                # Get hard pairs for this index
                batch_hard_pairs[i] = hard_pairs.get(dataset_idx_int, [])
            
            # Compute image features with gradients for HNML
            # Forward pass through image encoder - KEEP GRADIENTS for backward pass
            image_features = model.image_encoder(images.type(model.dtype))
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # The forward pass already computed and stored text features in model.text_features
            # We can directly use them for HNML
            if model.text_features is not None:
                # The text features shape might be [n_classes, feature_dim] instead of [batch_size, feature_dim]
                # Ensure we only use the first batch_size features if needed
                batch_size = image_features.size(0)
                if model.text_features.size(0) > batch_size:
                    text_features = model.text_features[:batch_size]
                else:
                    text_features = model.text_features
                
                # Verify shapes match for matrix operations
                if image_features.size(0) != text_features.size(0):
                    logger.warning(f"Shape mismatch in HNML: image_features={image_features.shape}, text_features={text_features.shape}")
                    # Fall back to just contrastive loss
                    loss = contrastive_loss
                    hnml = torch.tensor(0.0, device=device)
                    hnml_val = 0.0
                else:
                    # Compute Hard Negative Margin Loss
                    hnml = hard_negative_margin_loss(
                        image_features, 
                        text_features, 
                        batch_hard_pairs, 
                        alpha=hard_pair_miner.alpha
                    )
                    
                    # Make sure both losses are scalars
                    if isinstance(hnml, torch.Tensor) and hnml.dim() > 0:
                        hnml = hnml.mean()
                    
                    # Ensure we have scalar tensors with gradients for loss calculation
                    if isinstance(contrastive_loss, torch.Tensor) and not contrastive_loss.requires_grad:
                        logger.warning("Contrastive loss does not require gradients - fixing")
                        contrastive_loss = contrastive_loss.detach().clone().requires_grad_(True)
                    
                    # Combined loss (both components are now scalars)
                    loss = contrastive_loss + lambda_hnml * hnml
                    
                    # Get hnml value for logging (with no_grad to avoid memory leaks)
                    with torch.no_grad():
                        try:
                            hnml_val = hnml.item() if isinstance(hnml, torch.Tensor) and hnml.numel() == 1 else float(hnml)
                            total_hnml_loss += hnml_val
                        except Exception as e:
                            logger.warning(f"Error converting HNML to scalar: {e}")
                            hnml_val = 0.0
            else:
                # If we couldn't get text features, just use contrastive loss
                loss = contrastive_loss
                hnml = torch.tensor(0.0, device=device)
                hnml_val = 0.0
            
        else:
            loss = contrastive_loss
            hnml = torch.tensor(0.0, device=device)
            hnml_val = 0.0
        
        # Final check to ensure loss is a scalar with gradients before backward
        if isinstance(loss, torch.Tensor) and loss.dim() > 0:
            logger.warning(f"Final loss has shape {loss.shape} - converting to mean")
            loss = loss.mean()
        
        # Backward pass and optimize
        loss.backward()
        optimizer.step()
        
        # Safely get loss value for logging
        try:
            loss_val = loss.item() if hasattr(loss, 'item') else float(loss)
        except Exception as e:
            logger.warning(f"Error converting loss to scalar: {e}. Using sum of parts.")
            loss_val = cont_loss_val + hnml_val
            
        # Update metrics
        total_loss += loss_val
        
        # Update progress bar
        # HELIP
        if use_helip:
            progress_bar.set_postfix({
                'loss': f"{loss_val:.4f}",
                'cont_loss': f"{cont_loss_val:.4f}",
                'hnml': f"{hnml_val:.4f}" if lambda_hnml > 0 else "0.0000",
                'avg_loss': f"{total_loss / (batch_idx + 1):.4f}",
            })
            
            # Log to wandb
            wandb_run.log({
                "train_loss": loss_val,
                "contrastive_loss": cont_loss_val,
                "hnml": hnml_val if lambda_hnml > 0 else 0.0,
                "train_avg_loss": total_loss / (batch_idx + 1),
                "learning_rate": optimizer.param_groups[0]['lr']
            })
        else:
            progress_bar.set_postfix({
                'loss': f"{loss_val:.4f}",
                'avg_loss': f"{total_loss / (batch_idx + 1):.4f}",
            })
            
            # Log to wandb
            wandb_run.log({
                "train_loss": loss_val,
                "train_avg_loss": total_loss / (batch_idx + 1),
                "learning_rate": optimizer.param_groups[0]['lr']
            })
        # End of HELIP
    
    # Step the scheduler if provided
    if scheduler is not None:
        scheduler.step()
    
    # Return average losses
    avg_loss = total_loss / len(dataloader)
    avg_contrastive_loss = total_contrastive_loss / len(dataloader)
    
    # HELIP
    if use_helip:
        avg_hnml_loss = total_hnml_loss / len(dataloader) if lambda_hnml > 0 else 0.0
        
        logger.info(f"Epoch {epoch} - Training Loss: {avg_loss:.4f}, "
                    f"Contrastive Loss: {avg_contrastive_loss:.4f}, "
                    f"HNML: {avg_hnml_loss:.4f}")
    else:
        logger.info(f"Epoch {epoch} - Training Loss: {avg_loss:.4f}")
    # End of HELIP
    
    return avg_loss


@torch.no_grad()
def eval(model, dataset, categories, batch_size, device, tokenizer, label=""):
    model.eval()

    # Remap labels into a contiguous set starting from zero
    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}

    # here we apply the standard CLIP template used for oxford flowers to all categories
    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)
    text_inputs = tokenizer(
        [f"a photo of a {CLASS_NAMES[c]}, a type of flower." for c in categories]
    ).to(device)

    # we can encode the text features once as they are shared for all images
    # therefore we do it outside the evaluation loop
    text_features = model.encode_text(text_inputs)
    # and here we normalize them (standard pratice with CLIP)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # simple dataloader creation
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )

    # here we store the number of correct predictions we will make
    correct_predictions = 0
    for image, target in tqdm(dataloader, desc=label):
        # base categories range from 0 to 50, whil novel ones from 51 to 101
        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions
        # Map targets in contiguous set starting from zero
        # Labels needs to be .long() in pytorch
        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()

        image = image.to(device)
        target = target.to(device)

        # forward image through CLIP image encoder
        image_features = model.encode_image(image)
        # and normalize
        image_features /= image_features.norm(dim=-1, keepdim=True)

        # here cosine similarity between image and text features and keep the argmax for every row (every image)
        predicted_class = (image_features @ text_features.T).argmax(dim=-1)
        # now we check which are correct, and sum them (False == 0, True == 1)
        correct_predictions += (predicted_class == target).sum().item()

    # and now we compute the accuracy
    accuracy = correct_predictions / len(dataset)
    return accuracy


def harmonic_mean(base_accuracy, novel_accuracy):
    numerator = 2
    denominator = 1 / base_accuracy + 1 / novel_accuracy
    hm = numerator / denominator
    return hm


@torch.no_grad()
def evaluate(model, 
             dataloader, 
             device, 
             epoch, 
             split="test" #split=test? or split=val? Let's make sure we don't fuck up here, there was a question on this in the forum
             ):

    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    # Progress bar
    progress_bar = tqdm(dataloader, desc=f"{split.capitalize()} Epoch {epoch}")
    
    for batch_idx, (images, targets) in enumerate(progress_bar):
        # Move data to device
        images, targets = images.to(device), targets.to(device)
        
        # Forward pass
        outputs = model(images)
        loss = F.cross_entropy(outputs, targets)
        
        # Update metrics
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        # Calculate accuracy
        accuracy = 100. * correct / total
        
        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{accuracy:.2f}%"
        })
    
    # Calculate final metrics
    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total
    
    # Log to wandb
    wandb_run.log({
        f"{split}_loss": avg_loss,
        f"{split}_accuracy": accuracy,
        "epoch": epoch
    })
    
    logger.info(f"Epoch {epoch} - {split.capitalize()} Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
    return accuracy, avg_loss

# Evaluate model on both base and novel categories and compute harmonic mean
def eval_with_both_categories(custom_model, base_model, test_base_loader, test_novel_loader, 
                             base_classes, novel_classes, device, tokenizer, epoch):

    # Create dataloader for the metrics
    test_base_dataset = test_base_loader.dataset
    test_novel_dataset = test_novel_loader.dataset
    
    # Evaluate on base and novel categories
    base_accuracy = eval(
        model=custom_model if epoch > 0 else base_model,
        dataset=test_base_dataset,
        categories=base_classes,
        batch_size=128,
        device=device,
        tokenizer=tokenizer,
        label=f"Epoch {epoch} - Base Classes"
    )
    
    novel_accuracy = eval(
        model=custom_model if epoch > 0 else base_model,
        dataset=test_novel_dataset,
        categories=novel_classes,
        batch_size=128,
        device=device,
        tokenizer=tokenizer,
        label=f"Epoch {epoch} - Novel Classes"
    )
    
    # Calculate harmonic mean
    hm = harmonic_mean(base_accuracy, novel_accuracy)
    
    # Log results
    logger.info(f"Epoch {epoch} - Base Accuracy: {base_accuracy:.4f}, "
                f"Novel Accuracy: {novel_accuracy:.4f}, "
                f"Harmonic Mean: {hm:.4f}")
    
    # Log to wandb
    wandb_run.log({
        "base_accuracy": base_accuracy,
        "novel_accuracy": novel_accuracy,
        "harmonic_mean": hm,
        "epoch": epoch
    })
    
    return base_accuracy, novel_accuracy, hm


"""## Define model"""

class TextEncoder(torch.nn.Module):
    def __init__(self, clip_model, device):
        super().__init__()
        self.transformer = clip_model.transformer
        self.positional_embedding = clip_model.positional_embedding
        self.ln_final = clip_model.ln_final
        self.text_projection = clip_model.text_projection
        self.dtype = getattr(clip_model, "dtype", torch.float32)

    def forward(self, prompts, tokenized_prompts):
        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection

        return x


class PromptLearner(torch.nn.Module):
    def __init__(self, cfg, classnames, clip_model, tokenizer, device):
        super().__init__()
        n_cls = len(classnames)
        n_ctx = cfg["trainer"]["cocoop"]["n_ctx"]
        ctx_init = cfg["trainer"]["cocoop"]["ctx_init"]
        dtype = getattr(clip_model, "dtype", torch.float32)
        ctx_dim = clip_model.ln_final.weight.shape[0]
        vis_dim = clip_model.visual.output_dim
        clip_imsize = getattr(clip_model.visual, "input_resolution", 224)
        cfg_imsize = cfg["input"]["size"][0]
        assert cfg_imsize == clip_imsize, f"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})"

        if ctx_init:
            # use given words to initialize context vectors
            ctx_init = ctx_init.replace("_", " ")
            n_ctx = len(ctx_init.split(" "))
            prompt = tokenizer(ctx_init).to(device)
            with torch.no_grad():
                embedding = clip_model.token_embedding(prompt).type(dtype)
            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]
            prompt_prefix = ctx_init
        else:
            # random initialization
            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype, device=device)
            torch.nn.init.normal_(ctx_vectors, std=0.02)
            prompt_prefix = " ".join(["X"] * n_ctx)

        print(f'Initial context: "{prompt_prefix}"')
        print(f"Number of context words (tokens): {n_ctx}")

        self.ctx = torch.nn.Parameter(ctx_vectors)

        self.meta_net = torch.nn.Sequential(OrderedDict([
            ("linear1", torch.nn.Linear(vis_dim, vis_dim // 16)),
            ("relu", torch.nn.ReLU(inplace=True)),
            ("linear2", torch.nn.Linear(vis_dim // 16, ctx_dim))
        ])).to(device)
        
        if cfg["trainer"]["cocoop"]["prec"] == "fp16":
            self.meta_net.half()

        classnames = [name.replace("_", " ") for name in classnames]
        # Use tokenizer to encode classnames safely
        name_lens = [len(tokenizer(name)) for name in classnames]
        prompts = [prompt_prefix + " " + name + "." for name in classnames]

        # Move tokenized prompts to the correct device
        tokenized_prompts = torch.cat([tokenizer(p) for p in prompts]).to(device)  # (n_cls, n_tkn)
        with torch.no_grad():
            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)

        # These token vectors will be saved when in save_model(),
        # but they should be ignored in load_model() as we want to use
        # those computed using the current class names
        self.register_buffer("token_prefix", embedding[:, :1, :])  # SOS
        self.register_buffer("token_suffix", embedding[:, 1 + n_ctx :, :])  # CLS, EOS

        self.n_cls = n_cls
        self.n_ctx = n_ctx
        self.tokenized_prompts = tokenized_prompts  # torch.Tensor
        self.name_lens = name_lens
    
    def construct_prompts(self, ctx, prefix, suffix, label=None):
        # dim0 is either batch_size (during training) or n_cls (during testing)
        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)
        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)
        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)

        if label is not None:
            prefix = prefix[label]
            suffix = suffix[label]

        prompts = torch.cat(
            [
                prefix,  # (dim0, 1, dim)
                ctx,     # (dim0, n_ctx, dim)
                suffix,  # (dim0, *, dim)
            ],
            dim=1,
        )

        return prompts

    def forward(self, im_features):
        prefix = self.token_prefix
        suffix = self.token_suffix
        ctx = self.ctx                     # (n_ctx, ctx_dim)
        
        # Ensure consistent precision between meta_net and im_features (this caused me an error)
        if self.meta_net[0].weight.dtype != im_features.dtype:
            im_features = im_features.to(self.meta_net[0].weight.dtype)
            
        bias = self.meta_net(im_features)  # (batch, ctx_dim)
        bias = bias.unsqueeze(1)           # (batch, 1, ctx_dim)
        ctx = ctx.unsqueeze(0)             # (1, n_ctx, ctx_dim)
        ctx_shifted = ctx + bias           # (batch, n_ctx, ctx_dim)
        
        # Use instance-conditioned context tokens for all classes
        prompts = []
        for ctx_shifted_i in ctx_shifted:
            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)
            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)
            prompts.append(pts_i)
        prompts = torch.stack(prompts)
        
        return prompts


class CustomCLIP(torch.nn.Module):
    def __init__(self, cfg, classnames, clip_model, tokenizer, device):
        super().__init__()
        self.prompt_learner = PromptLearner(cfg, classnames, clip_model, tokenizer, device) #don't we need this tokenizer here?
        self.tokenized_prompts = self.prompt_learner.tokenized_prompts
        self.image_encoder = clip_model.visual
        self.text_encoder = TextEncoder(clip_model, device)
        self.logit_scale = clip_model.logit_scale
        self.dtype = getattr(clip_model, "dtype", torch.float32)
        self.device = device
        # HELIP: Store the text features for later use
        self.text_features = None
    
    def forward(self, image, label=None):
        tokenized_prompts = self.tokenized_prompts
        logit_scale = self.logit_scale.exp()

        # Use the default precision for image features
        image_features = self.image_encoder(image.type(self.dtype))
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        # If meta_net is using half precision, convert image_features to half precision for the prompt learner
        if getattr(self.prompt_learner, 'meta_net', None) is not None and next(self.prompt_learner.meta_net.parameters()).dtype == torch.float16:
            prompts = self.prompt_learner(image_features.half())
        else:
            prompts = self.prompt_learner(image_features)
        
        logits = []
        
        # HELIP: Store all text features
        all_text_features = []
        
        for pts_i, imf_i in zip(prompts, image_features):
            text_features = self.text_encoder(pts_i, tokenized_prompts)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            # HELIP: Store text features
            all_text_features.append(text_features)
            
            l_i = logit_scale * imf_i @ text_features.t()
            logits.append(l_i)
        
        logits = torch.stack(logits)
        
        # HELIP: Store text features for later use in compute_similarity
        self.text_features = torch.cat(all_text_features)
        
        if self.prompt_learner.training:
            if label is not None:
                # Check if logits shape is compatible with cross_entropy
                if logits.dim() > 2:
                    # Reshape logits to [batch_size, num_classes]
                    batch_size = image.size(0)
                    logits = logits.view(batch_size, -1)
                    
                # Ensure label shape matches what cross_entropy expects
                if label.dim() > 1:
                    label = label.view(-1)
                
                # Try to compute cross entropy, with fallback
                try:
                    return F.cross_entropy(logits, label)
                except Exception as e:
                    # Fallback: compute mean of each logit row as a scalar loss
                    logger.warning(f"Cross entropy failed: {e}. Using fallback loss.")
                    return logits.mean()
            else:
                # If no label, return mean of logits as scalar loss during training
                return logits.mean()
        
        return logits
    
    # Compute the similarity between image features and text features to make HELIP work
    def compute_similarity(self, image_features):
        # Ensure text features exist
        if self.text_features is None:
            raise ValueError("Text features not computed yet. Call forward method first.")
        
        # Get logit scale
        logit_scale = self.logit_scale.exp()
        
        # Compute similarity scores
        logits_per_image = logit_scale * (image_features @ self.text_features.T)
        logits_per_text = logits_per_image.T
        
        return logits_per_image, logits_per_text


def create_base_model(device):
    result = open_clip.create_model_from_pretrained(
        model_name=cfg["base_model"]["name"],
        pretrained=cfg["base_model"]["weights"],
        device=device,
        return_transform=True,
    )
    assert isinstance(result, tuple)
    model, preprocess = result
    tokenizer = open_clip.get_tokenizer(cfg["base_model"]["name"])
    return model, preprocess, tokenizer


def create_custom_model(device):
    base_model, preprocess, tokenizer = create_base_model(device)
    model = CustomCLIP(cfg, CLASS_NAMES, base_model, tokenizer, device)
    return model, preprocess, tokenizer #adding tokenizer in line with "self.prompt_learner" definiton

"""## Initialize model"""

device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"Using {device} device")

base_model, base_preprocess, base_tokenizer = create_base_model(device)
custom_model, custom_preprocess, custom_tokenizer = create_custom_model(device)
print(base_model)

"""# Load and prepare data

"""

# Create data augmentation for training 
# TO DO: Explore better augmentations, this is just boiler-plate 
train_transform = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(size=224, scale=(0.7, 1.0)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
    base_preprocess
])

# get the three datasets
train_set, val_set, test_set = get_data(
    train_transform=train_transform,  # Apply augmentations to training set
    test_transform=base_preprocess    # Only basic preprocessing for val/test
)

# split classes into base and novel
base_classes, novel_classes = base_novel_categories(train_set)

# split the three datasets
train_base, _ = split_data(train_set, base_classes)
val_base, _ = split_data(val_set, base_classes)
test_base, test_novel = split_data(test_set, base_classes)

"""## Train model"""

# Define optimizer and LR scheduler.
def get_optimizer_and_scheduler(model, cfg):

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg["trainer"]["lr"],
        weight_decay=cfg["trainer"]["weight_decay"]
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg["trainer"]["epochs"],
        eta_min=1e-6
    )
    
    # Use warmup
    if cfg["trainer"]["warmup_epochs"] > 0:
        scheduler = torch.optim.lr_scheduler.LinearLR( # TO DO: Explore better warmups
            optimizer, 
            start_factor=0.01,
            end_factor=1.0,
            total_iters=cfg["trainer"]["warmup_epochs"]
        )
    
    return optimizer, scheduler


# Create PyTorch DataLoaders for training and evaluation + test_base, and test_novel.
def create_data_loaders(train_dataset,
                        val_dataset, 
                        test_base_dataset, 
                        test_novel_dataset, 
                        batch_size, 
                        num_workers=4, 
                        pin_memory=True):

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    test_base_loader = DataLoader(
        test_base_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    test_novel_loader = DataLoader(
        test_novel_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    logger.info(f"Created dataloaders with batch size {batch_size}")
    return train_loader, val_loader, test_base_loader, test_novel_loader


def save_checkpoint(model, 
                    optimizer, 
                    scheduler, 
                    epoch, 
                    accuracy, 
                    best_acc, 
                    checkpoint_dir, 
                    is_best=False):

    checkpoint = {
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'scheduler': scheduler.state_dict() if scheduler else None,
        'epoch': epoch,
        'accuracy': accuracy,
        'best_accuracy': best_acc
    }
    
    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"Saved checkpoint at {checkpoint_path}")
    
    if is_best:
        best_path = os.path.join(checkpoint_dir, 'model_best.pth')
        torch.save(checkpoint, best_path)
        logger.info(f"Saved best model with accuracy {accuracy:.4f} at {best_path}")


def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device):
    try:
        logger.info(f"Loading checkpoint from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        
        if scheduler and checkpoint['scheduler']:
            scheduler.load_state_dict(checkpoint['scheduler'])
        
        epoch = checkpoint['epoch']
        best_accuracy = checkpoint.get('best_accuracy', 0.0)
        
        logger.info(f"Loaded checkpoint from epoch {epoch} with accuracy {checkpoint.get('accuracy', 0.0):.4f}")
        return epoch, best_accuracy
    except Exception as e:
        logger.error(f"Error loading checkpoint: {e}")
        return 0, 0.0


def train_cocoop(cfg, device):
    # Check if HELIP is enabled
    helip_enabled = cfg.get("helip", {}).get("enabled", False)
    
    if helip_enabled:
        logger.info("==== Starting CoCoOp Training with HELIP ====")
    else:
        logger.info("==== Starting CoCoOp Training ====")
    # End of HELIP part
        
    logger.info(f"Config: {cfg}")
    
    # Set random seed for reproducibility
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    # Create models
    logger.info("Creating models...")
    base_model, base_preprocess, tokenizer = create_base_model(device)
    custom_model, _, _ = create_custom_model(device)
    custom_model = custom_model.to(device)
    
    # Create data augmentation for training
    train_transform = torchvision.transforms.Compose([
        torchvision.transforms.RandomResizedCrop(size=224, scale=(0.7, 1.0)),
        torchvision.transforms.RandomHorizontalFlip(),
        torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
        base_preprocess
    ])
    
    # Get datasets with transforms
    logger.info("Loading datasets...")
    train_set, val_set, test_set = get_data(
        data_dir=cfg["data"]["data_dir"],
        train_transform=train_transform,
        test_transform=base_preprocess
    )
    
    # Split base and novel classes
    base_classes, novel_classes = base_novel_categories(train_set)
    
    # Split datasets
    train_base, _ = split_data(train_set, base_classes)
    val_base, _ = split_data(val_set, base_classes)
    test_base, test_novel = split_data(test_set, base_classes)
    
    # Create dataloaders
    logger.info("Creating dataloaders...")
    train_loader, val_loader, test_base_loader, test_novel_loader = create_data_loaders(
        train_base, val_base, test_base, test_novel,
        batch_size=cfg["trainer"]["batch_size"],
        num_workers=cfg["data"]["num_workers"],
        pin_memory=cfg["data"]["pin_memory"]
    )
    
    # Create optimizer and scheduler
    logger.info("Setting up optimizer and scheduler...")
    optimizer, scheduler = get_optimizer_and_scheduler(custom_model, cfg)
    
    # Initialize HELIP if enabled
    hard_pair_miner = None
    if helip_enabled:
        logger.info("HELIP is enabled for training")
        # Initialize hard pair mining for base classes only
        hard_pair_miner = HardPairMining(
            custom_clip_model=custom_model,
            dataset=train_base,
            device=device,
            cfg=cfg
        )
    else:
        logger.info("HELIP is disabled, using standard training")
    
    # Training setup
    start_epoch = 0
    best_accuracy = 0.0
    best_base_acc = 0.0
    best_novel_acc = 0.0
    best_hm = 0.0
    patience_counter = 0
    
    # Check for resume training
    checkpoint_path = os.path.join(cfg["trainer"]["checkpoint_dir"], "model_best.pth")
    if os.path.exists(checkpoint_path):
        logger.info(f"Found checkpoint at {checkpoint_path}, resuming training...")
        start_epoch, best_accuracy = load_checkpoint(
            custom_model, optimizer, scheduler, checkpoint_path, device
        )
        start_epoch += 1  # Start from the next epoch
    
    # Evaluate zero-shot performance of the base model (as a baseline, before any training)
    logger.info("Evaluating zero-shot performance of base model...")
    base_zero_shot_acc, novel_zero_shot_acc, zero_shot_hm = eval_with_both_categories(
        custom_model=custom_model,
        base_model=base_model,
        test_base_loader=test_base_loader,
        test_novel_loader=test_novel_loader,
        base_classes=base_classes,
        novel_classes=novel_classes,
        device=device,
        tokenizer=tokenizer,
        epoch=0
    )
    
    logger.info(f"Zero-shot: Base Acc={base_zero_shot_acc:.4f}, "
                f"Novel Acc={novel_zero_shot_acc:.4f}, "
                f"HM={zero_shot_hm:.4f}")
    
    # Training loop
    logger.info("Starting training...")
    for epoch in range(start_epoch, cfg["trainer"]["epochs"]):
        try:
            # Train for one epoch with unified train_loop function
            train_loss = train_loop(
                dataloader=train_loader,
                model=custom_model,
                optimizer=optimizer,
                epoch=epoch + 1,
                device=device,
                scheduler=scheduler,
                use_helip=helip_enabled,
                hard_pair_miner=hard_pair_miner,
                lambda_hnml=cfg["helip"].get("lambda_hnml", 0.5) if helip_enabled else 0.0
            )
            
            # Evaluate on validation set
            val_acc, val_loss = evaluate(
                model=custom_model,
                dataloader=val_loader,
                device=device,
                epoch=epoch + 1,
                split="val"
            )
            
            # Evaluate on test sets (base and novel)
            base_acc, novel_acc, hm = eval_with_both_categories(
                custom_model=custom_model,
                base_model=base_model,
                test_base_loader=test_base_loader,
                test_novel_loader=test_novel_loader,
                base_classes=base_classes,
                novel_classes=novel_classes,
                device=device,
                tokenizer=tokenizer,
                epoch=epoch + 1
            )
            
            # Check if this is the best model (by harmonic mean)
            is_best = hm > best_hm
            if is_best:
                best_hm = hm
                best_base_acc = base_acc
                best_novel_acc = novel_acc
                patience_counter = 0
            else:
                patience_counter += 1
            
            # Save checkpoints
            save_checkpoint(
                model=custom_model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch + 1,
                accuracy=hm,
                best_acc=best_hm,
                checkpoint_dir=cfg["trainer"]["checkpoint_dir"],
                is_best=is_best
            )
            
            # Early stopping
            if patience_counter >= cfg["trainer"]["patience"]:
                logger.info(f"Early stopping triggered after {patience_counter} epochs without improvement")
                break
            
        except Exception as e:
            logger.error(f"Error during training: {e}")
            # Save emergency checkpoint
            save_checkpoint(
                model=custom_model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch + 1,
                accuracy=best_hm,
                best_acc=best_hm,
                checkpoint_dir=cfg["trainer"]["checkpoint_dir"],
                is_best=False
            )
            raise e
    
    # Load best model for final evaluation
    best_model_path = os.path.join(cfg["trainer"]["checkpoint_dir"], "model_best.pth")
    if os.path.exists(best_model_path):
        _, _ = load_checkpoint(custom_model, optimizer, None, best_model_path, device)
    
    # Final evaluation
    logger.info("==== Final Evaluation ====")
    final_base_acc, final_novel_acc, final_hm = eval_with_both_categories(
        custom_model=custom_model,
        base_model=base_model,
        test_base_loader=test_base_loader,
        test_novel_loader=test_novel_loader,
        base_classes=base_classes,
        novel_classes=novel_classes,
        device=device,
        tokenizer=tokenizer,
        epoch=cfg["trainer"]["epochs"] + 1
    )
    
    logger.info(f"Final: Base Acc={final_base_acc:.4f}, "
                f"Novel Acc={final_novel_acc:.4f}, "
                f"HM={final_hm:.4f}")
    
    # Log improvement over zero-shot
    logger.info(f"Improvement over zero-shot: "
                f"Base: {(final_base_acc - base_zero_shot_acc) * 100:.2f}%, "
                f"Novel: {(final_novel_acc - novel_zero_shot_acc) * 100:.2f}%, "
                f"HM: {(final_hm - zero_shot_hm) * 100:.2f}%")
    
    # Return best results
    return best_base_acc, best_novel_acc, best_hm
# End of HELIP part

def main():
    """Main function that ties everything together."""
    try:
        # Set device
        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
        logger.info(f"Using {device} device")
        
        # Initial zero-shot evaluation with the base model
        base_model, base_preprocess, base_tokenizer = create_base_model(device)
        
        # Create data augmentation for training
        train_transform = torchvision.transforms.Compose([
            torchvision.transforms.RandomResizedCrop(size=224, scale=(0.7, 1.0)),
            torchvision.transforms.RandomHorizontalFlip(),
            torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
            base_preprocess
        ])
        
        # Get datasets
        train_set, val_set, test_set = get_data(
            data_dir=cfg["data"]["data_dir"], 
            train_transform=train_transform,  # Apply augmentations to training set
            test_transform=base_preprocess    # Only basic preprocessing for val/test
        )
        
        # Split classes
        base_classes, novel_classes = base_novel_categories(train_set)
        
        # Split datasets
        train_base, _ = split_data(train_set, base_classes)
        val_base, _ = split_data(val_set, base_classes)
        test_base, test_novel = split_data(test_set, base_classes)
        
        # Initial zero-shot evaluation (this was previously set, I've just moved it a bit)
        logger.info("Running initial zero-shot evaluation...")
        base_accuracy = eval(
            model=base_model,
            dataset=test_base,
            categories=base_classes,
            batch_size=128,
            device=device,
            tokenizer=base_tokenizer,
            label="🧠 Zero-shot evaluation on Base Classes",
        )
        novel_accuracy = eval(
            model=base_model,
            dataset=test_novel,
            categories=novel_classes,
            batch_size=128,
            device=device,
            tokenizer=base_tokenizer,
            label="🧠 Zero-shot evaluation on Novel Classes",
        )
        
        hm = harmonic_mean(base_accuracy, novel_accuracy)
        
        logger.info("==== Zero-shot Evaluation Results ====")
        logger.info(f"🔍 Base classes accuracy: {base_accuracy * 100:.2f}%")
        logger.info(f"🔍 Novel classes accuracy: {novel_accuracy * 100:.2f}%")
        logger.info(f"🔍 Harmonic Mean: {hm * 100:.2f}%")
        
        # Run training with HELIP configured via config
        logger.info("Starting training...")
        best_base_acc, best_novel_acc, best_hm = train_cocoop(cfg, device)
        # End of HELIP addition
        
        # Final comparison
        logger.info("==== Improvement Summary ====")
        logger.info(f"Base classes: {base_accuracy * 100:.2f}% -> {best_base_acc * 100:.2f}% "
                    f"(+{(best_base_acc - base_accuracy) * 100:.2f}%)")
        logger.info(f"Novel classes: {novel_accuracy * 100:.2f}% -> {best_novel_acc * 100:.2f}% "
                    f"(+{(best_novel_acc - novel_accuracy) * 100:.2f}%)")
        logger.info(f"Harmonic Mean: {hm * 100:.2f}% -> {best_hm * 100:.2f}% "
                    f"(+{(best_hm - hm) * 100:.2f}%)")
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise e
    finally:
        # Cleanup
        logger.info("Finishing wandb run...")
        wandb_run.finish()


if __name__ == "__main__":
    main()
