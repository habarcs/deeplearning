Using cuda device
2025-05-12 17:18:01,684 - root - INFO - Loaded ViT-B-32 model config.
2025-05-12 17:18:03,466 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2025-05-12 17:18:03,706 - root - INFO - Loaded ViT-B-32 model config.
2025-05-12 17:18:05,167 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
2025-05-12 17:18:05,451 - cocoop_training - INFO - Loading Flowers102 dataset from ./data
2025-05-12 17:18:05,476 - cocoop_training - INFO - Dataset loaded: 1020 train, 1020 val, 6149 test
2025-05-12 17:18:05,476 - cocoop_training - INFO - Using cuda device
2025-05-12 17:18:05,477 - root - INFO - Loaded ViT-B-32 model config.
2025-05-12 17:18:06,584 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2025-05-12 17:18:06,782 - cocoop_training - INFO - Loading Flowers102 dataset from ./data
2025-05-12 17:18:06,796 - cocoop_training - INFO - Dataset loaded: 1020 train, 1020 val, 6149 test
2025-05-12 17:18:06,797 - cocoop_training - INFO - Running initial zero-shot evaluation...
 ... (more hidden) ...
 ... (more hidden) ...
2025-05-12 17:21:34,638 - cocoop_training - INFO - ==== Zero-shot Evaluation Results ====
2025-05-12 17:21:34,638 - cocoop_training - INFO - üîç Base classes accuracy: 74.69%
2025-05-12 17:21:34,638 - cocoop_training - INFO - üîç Novel classes accuracy: 77.04%
2025-05-12 17:21:34,638 - cocoop_training - INFO - üîç Harmonic Mean: 75.85%
2025-05-12 17:21:34,638 - cocoop_training - INFO - Starting CoCoOp training...
2025-05-12 17:21:34,638 - cocoop_training - INFO - ==== Starting CoCoOp Training ====
2025-05-12 17:21:34,638 - cocoop_training - INFO - Config: {'base_model': {'name': 'ViT-B-32', 'weights': 'laion2b_s34b_b79k'}, 'trainer': {'batch_size': 8, 'epochs': 3, 'lr': 0.002, 'weight_decay': 0.05, 'warmup_epochs': 1, 'patience': 5, 'checkpoint_dir': './checkpoints', 'cocoop': {'n_ctx': 16, 'ctx_init': '', 'prec': 'fp16'}}, 'input': {'size': [224, 224]}, 'data': {'data_dir': './data', 'num_workers': 2, 'pin_memory': True}}
2025-05-12 17:21:34,640 - cocoop_training - INFO - Creating models...
2025-05-12 17:21:34,640 - root - INFO - Loaded ViT-B-32 model config.
2025-05-12 17:21:36,340 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2025-05-12 17:21:36,636 - root - INFO - Loaded ViT-B-32 model config.
2025-05-12 17:21:38,642 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
2025-05-12 17:21:38,937 - cocoop_training - INFO - Loading datasets...
2025-05-12 17:21:38,937 - cocoop_training - INFO - Loading Flowers102 dataset from ./data
2025-05-12 17:21:38,965 - cocoop_training - INFO - Dataset loaded: 1020 train, 1020 val, 6149 test
2025-05-12 17:21:38,971 - cocoop_training - INFO - Creating dataloaders...
2025-05-12 17:21:38,972 - cocoop_training - INFO - Created dataloaders with batch size 8
2025-05-12 17:21:38,972 - cocoop_training - INFO - Setting up optimizer and scheduler...
2025-05-12 17:21:38,973 - cocoop_training - INFO - Evaluating zero-shot performance of base model...
 ... (more hidden) ...
 ... (more hidden) ...
2025-05-12 17:24:04,336 - cocoop_training - INFO - Epoch 0 - Base Accuracy: 0.7469, Novel Accuracy: 0.7704, Harmonic Mean: 0.7585
2025-05-12 17:24:04,336 - cocoop_training - INFO - Zero-shot: Base Acc=0.7469, Novel Acc=0.7704, HM=0.7585
2025-05-12 17:24:04,337 - cocoop_training - INFO - Starting training...
 ... (more hidden) ...
2025-05-12 17:25:12,308 - cocoop_training - ERROR - Error during training: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 18.08 GiB is allocated by PyTorch, and 305.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-12 17:25:12,901 - cocoop_training - ERROR - Error in main execution: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-05-12 17:25:12,902 - cocoop_training - INFO - Finishing wandb run...
