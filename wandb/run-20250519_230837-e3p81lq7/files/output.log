Using cuda device
2025-05-19 23:08:38,695 - root - INFO - Loaded ViT-B-32 model config.
2025-05-19 23:08:40,633 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2025-05-19 23:08:40,874 - root - INFO - Loaded ViT-B-32 model config.
2025-05-19 23:08:42,644 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
Initial context: "X X X X X X X X"
Number of context words (tokens): 8
CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
2025-05-19 23:08:42,974 - cocoop_training - INFO - Loading Flowers102 dataset from ./data
2025-05-19 23:08:43,014 - cocoop_training - INFO - Dataset loaded: 1020 train, 1020 val, 6149 test
2025-05-19 23:08:43,015 - cocoop_training - INFO - Using cuda device
2025-05-19 23:08:43,015 - root - INFO - Loaded ViT-B-32 model config.
2025-05-19 23:08:46,436 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2025-05-19 23:08:46,901 - cocoop_training - INFO - Loading Flowers102 dataset from ./data
2025-05-19 23:08:46,922 - cocoop_training - INFO - Dataset loaded: 1020 train, 1020 val, 6149 test
2025-05-19 23:08:46,923 - cocoop_training - INFO - Running initial zero-shot evaluation...
ðŸ§  Zero-shot evaluation on Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:58<00:00,  2.92s/it]
ðŸ§  Zero-shot evaluation on Novel Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [02:05<00:00,  4.33s/it]
2025-05-19 23:11:51,586 - cocoop_training - INFO - ==== Zero-shot Evaluation Results ====
2025-05-19 23:11:51,587 - cocoop_training - INFO - ðŸ” Base classes accuracy: 74.69%
2025-05-19 23:11:51,587 - cocoop_training - INFO - ðŸ” Novel classes accuracy: 77.04%
2025-05-19 23:11:51,587 - cocoop_training - INFO - ðŸ” Harmonic Mean: 75.85%
2025-05-19 23:11:51,587 - cocoop_training - INFO - Starting training...
2025-05-19 23:11:51,587 - cocoop_training - INFO - ==== Starting CoCoOp Training with HELIP ====
2025-05-19 23:11:51,588 - cocoop_training - INFO - Config: {'base_model': {'name': 'ViT-B-32', 'weights': 'laion2b_s34b_b79k'}, 'trainer': {'batch_size': 4, 'epochs': 3, 'lr': 0.002, 'weight_decay': 0.05, 'warmup_epochs': 1, 'patience': 5, 'checkpoint_dir': './checkpoints', 'cocoop': {'n_ctx': 8, 'ctx_init': '', 'prec': 'fp16'}}, 'input': {'size': [224, 224]}, 'data': {'data_dir': './data', 'num_workers': 0, 'pin_memory': False}, 'helip': {'enabled': True, 'k': 5, 'p': 20, 'alpha': 0.25, 'lambda_hnml': 0.5, 'mining_freq': 5, 'cache_embeddings': True}}
2025-05-19 23:11:51,591 - cocoop_training - INFO - Creating models...
2025-05-19 23:11:51,591 - root - INFO - Loaded ViT-B-32 model config.
2025-05-19 23:11:53,411 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2025-05-19 23:11:53,658 - root - INFO - Loaded ViT-B-32 model config.
2025-05-19 23:11:55,408 - root - INFO - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
Initial context: "X X X X X X X X"
Number of context words (tokens): 8
2025-05-19 23:11:55,727 - cocoop_training - INFO - Loading datasets...
2025-05-19 23:11:55,727 - cocoop_training - INFO - Loading Flowers102 dataset from ./data
2025-05-19 23:11:55,757 - cocoop_training - INFO - Dataset loaded: 1020 train, 1020 val, 6149 test
2025-05-19 23:11:55,763 - cocoop_training - INFO - Creating dataloaders...
2025-05-19 23:11:55,764 - cocoop_training - INFO - Created dataloaders with batch size 4
2025-05-19 23:11:55,764 - cocoop_training - INFO - Setting up optimizer and scheduler...
2025-05-19 23:11:55,766 - cocoop_training - INFO - HELIP is enabled for training
2025-05-19 23:11:55,768 - faiss.loader - INFO - Loading faiss with AVX2 support.
2025-05-19 23:11:55,796 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.
2025-05-19 23:11:55,802 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-19 23:11:55,803 - cocoop_training - INFO - FAISS available for efficient hard pair mining
2025-05-19 23:11:55,803 - cocoop_training - INFO - Evaluating zero-shot performance of base model...
Epoch 0 - Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:19<00:00,  3.99s/it]
Epoch 0 - Novel Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:53<00:00,  1.83s/it]
2025-05-19 23:14:10,807 - cocoop_training - INFO - Epoch 0 - Base Accuracy: 0.7469, Novel Accuracy: 0.7704, Harmonic Mean: 0.7585
2025-05-19 23:14:10,808 - cocoop_training - INFO - Zero-shot: Base Acc=0.7469, Novel Acc=0.7704, HM=0.7585
2025-05-19 23:14:10,808 - cocoop_training - INFO - Starting training...
Train Epoch 1:   0%|                                                                                     | 0/127 [00:00<?, ?it/s]2025-05-19 23:14:10,810 - cocoop_training - INFO - Computing embeddings for hard pair mining
Computing embeddings for HELIP: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [07:49<00:00,  3.67s/it]
Mining hard pairs for epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [00:00<00:00, 3903.39it/s]
Train Epoch 1:   0%|                                                                                     | 0/127 [08:51<?, ?it/s]
2025-05-19 23:23:02,356 - cocoop_training - ERROR - Error during training: a Tensor with 408 elements cannot be converted to Scalar
2025-05-19 23:23:03,661 - cocoop_training - INFO - Saved checkpoint at ./checkpoints/checkpoint_epoch_1.pth
2025-05-19 23:23:03,662 - cocoop_training - ERROR - Error in main execution: a Tensor with 408 elements cannot be converted to Scalar
2025-05-19 23:23:03,662 - cocoop_training - INFO - Finishing wandb run...
